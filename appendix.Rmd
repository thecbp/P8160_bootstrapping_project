---
title: "Appendix"
author: "Christian Pascual"
date: "4/16/2019"
output: pdf_document
---

```{r data, message = FALSE, warning = FALSE }
library(tidyverse)
library(glmnet)
library(parallel) 
library(doParallel) 
library(foreach) 
library(iterators)
source("LogLasso.R")

nCores = 6 # change to whatever fits your specific system
registerDoParallel(nCores)

standardize = function(col) {
  mean = mean(col)
  sd = sd(col)
  return((col - mean)/sd)
}

ds = read.csv("Down.csv") %>% 
  mutate(
    class = ifelse(Class == "Control", 0, 1)
    ) %>% 
  select(-MouseID, -Class) %>% 
  na.omit() # getting rid of nas since glmnet cannot handle missing data

ds.X = ds %>% select(-class) %>% map_df(.x = ., standardize)
ds.X = ds.X %>% mutate(intercept = 1) %>% select(intercept, everything())
ds.y = ds$class
```

```{r}
start = rep(0.01, 78)
glm.cv = cv.glmnet(as.matrix(ds.X[,-1]), ds.y)
glm.fit = glmnet(as.matrix(ds.X[,-1]), ds.y, family = "binomial", alpha = 1, 
                 lambda = glm.cv$lambda.min)
check.LL = reglog(ds.X, ds.y, start, lambda = glm.cv$lambda.min, 
                  tol = 1e-5, print = TRUE)
coef.comp = tibble(
  glm = matrix(coef(glm.fit)),
  regll = check.LL$coefficients
)
View(coef.comp)
plot(glm.cv)
```

```{r}
set.seed(8160)
lambdas = exp(seq(-8, 0, length.out = 100))
lambda.path = lambda.cv(lambdas, start, ds.X, ds.y)

path.min.idx = which(lambda.path$avg.fold.mse == min(lambda.path$avg.fold.mse))
path.min.loglambda = lambda.path$log.lambda[path.min.idx]
path.min.lambda = lambda.path$lambda[path.min.idx]

ggplot(data = lambda.path, aes(x = log.lambda, y = avg.fold.mse)) + 
  geom_point() +
  geom_vline(xintercept = path.min.loglambda)
```

```{r}
plot(glm.cv)
```


```{r}


start = rep(0, 78)
lambdas = exp(seq(-8, -1, length.out = 100))
sol.path = create.sol.path(lambdas, start, ds.X, ds.y)

tidy.path = as.tibble(sol.path) %>% 
  gather(., key = "coeff", value = "coeff_est", intercept:CaNA_N) %>% 
  mutate(log.lambda = log(lambda))

ggplot(data = tidy.path, aes(x = log.lambda, y = coeff_est, color = coeff, group = coeff)) +
  geom_line(alpha = 0.5) +
  labs(
    title = "Log-LASSO Coefficient estimates as a function of log(lambda)",
    x = "log(lambda)",
    y = "Coefficient estimate"
  ) +
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5))
```



# Setup

```{r setup, eval = FALSE }
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet)
library(doParallel)
source("LogLasso.R")
cl<-makeCluster(2) #change the 2 to your number of CPU cores  
registerDoParallel(cl)  
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
```

# Reading and cleaning the data
```{r, eval = FALSE }
down_dat = read_csv("Down.csv") %>% select(-MouseID) %>% mutate(Class = as.factor(Class)) %>% 
  na.omit() #getting rid of nas since glmnet cannot handle missing data
y_full = as.matrix(ifelse(down_dat$Class == "Control",0,1))
x_full = as.matrix(down_dat %>% select(-Class))
set.seed(1001)
#Geting the trained dataset
train_index = sample(1:nrow(x_full), ceiling(nrow(x_full) * 0.9))
y_train = y_full[train_index]
x_train = x_full[train_index,]
#getting the test dataset
y_test = y_full[-train_index]
x_test = x_full[-train_index,]
```

# Fitting and predicting using our model and built in lasso regression function

```{r, eval = FALSE}
#Our model
start = Sys.time()
log_lasso = optimize(as.data.frame(x_train),y_train, lambda_seq, rep(0.01, 77))
our_model = log_lasso$finalModel
y_predict_our_model = predict(our_model,x_test)
y_response_our_model = as.numeric(y_predict_our_model > 0.5)
end = Sys.time()
#reuslts
accuracy_our_model = mean(y_response_our_model == y_test)
mse_our_model = mean((y_predict_our_model - y_test)^2)
time_our_model = end - start
lambda_our_model = log_lasso$lambda.min
#Compare with the built in lasso
start = Sys.time()
lasso_glmnet_cv = cv.glmnet(x_train, y_train, alpha = 1, family = "binomial", lambda = lambda_seq)
y_response_glmnet = predict.cv.glmnet(lasso_glmnet_cv, newx = x_test, type = "response", s = "lambda.min")
end = Sys.time()
#results
accuracy_glmnet = mean((y_response_glmnet>0.5) == y_test) 
mse_glmnet = mean((y_response_glmnet - y_test)^2)
time_glmnet = end - start
lambda_glmnet = lasso_glmnet_cv$lambda.min
```

# Building the bootstrap-smoothing function
```{r, eval = FALSE }
boot_smooth <- function(x_train,y_train,x_test,y_test,lambda_seq,iterations=100){
  predictions <- foreach(m = 1:iterations,.combine = cbind, .packages = "glmnet") %dopar% {
    boot_positions <- sample(nrow(x_train), size = nrow(x_train), replace = T)
    boot_pos <- 1:nrow(x_train) %in% boot_positions
    y = y_train[boot_pos]
    x = x_train[boot_pos,]
    cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial", lambda = lambda_seq)
    # Fit the final model on the training data
    model <- glmnet(x, y, alpha = 1, family = "binomial",
                lambda = cv.lasso$lambda.min)
    predict(model,newx = x_test, type = "response")
  }
  apply(predictions, 1, mean) #Getting the mean
}

```

# Prediction for bootstrap-smoothing approach
```{r, eval = FALSE }
start = Sys.time()
y_response_boot = boot_smooth(x_train,y_train,x_test,y_test, lambda_seq = lambda_seq, iterations = 100)
end = Sys.time()
#results
accuracy_boot = mean((y_response_boot > 0.5) == y_test)
mse_boot = mean((y_response_boot - y_test)^2)
time_boot = end - start
lambda_boot = NA
```

# Results comparing the three methods
```{r, eval = FALSE }
#Build a result table
result_frame = data.frame(model = c("Our lasso", "Built in cv lasso", "Boot smooth mehtod"), accuracy = c(accuracy_our_model, accuracy_glmnet, accuracy_boot),
           MSE = c(mse_our_model, mse_glmnet, mse_boot), time = c(time_our_model,time_glmnet,time_boot),
           lambda_min = c(lambda_our_model, lambda_glmnet, lambda_boot))
knitr::kable(result_frame)
```

```{r, eval = FALSE }
# Set the index for 5 fold cross-validation
set.seed(1000)
n_folds <- 4
folds_i <- sample(rep(1:n_folds, length.out = nrow(x_full)))
result_pool = data.frame()
for (boot_n in c(50,100,200,300)) {#Number of bootstrap samples for smooth bootstraping
  cv_tmp <- matrix(NA, nrow = n_folds, ncol = 4)
  for (k in 1:n_folds) {
    #getting the train and test dataset for this fold
    test_i <- which(folds_i == k)
    x_test = x_full[test_i,]
    y_test = y_full[test_i]
    x_train = x_full[-test_i,]
    y_train = y_full[-test_i]
    
    #Laso
    lasso_glmnet_cv = cv.glmnet(x_train, y_train, alpha = 1, family = "binomial", lambda = lambda_seq)
    y_response_glmnet = predict.cv.glmnet(lasso_glmnet_cv, newx = x_test, type = "response", s = "lambda.min")
    #results
    accuracy_glmnet = mean((y_response_glmnet>0.5) == y_test) 
    mse_glmnet = mean((y_response_glmnet - y_test)^2)
    
    #bootstrap smooth
    y_response_boot = boot_smooth(x_train,y_train,x_test,y_test, lambda_seq = lambda_seq, iterations = boot_n)
    #results
    accuracy_boot = mean((y_response_boot > 0.5) == y_test)
    mse_boot = mean((y_response_boot - y_test)^2)
    
    #Building the result dataframe
    cv_tmp[k, 1] = accuracy_glmnet
    cv_tmp[k, 2] = accuracy_boot
    cv_tmp[k, 3] = mse_glmnet
    cv_tmp[k, 4] = mse_boot
  }
  result_cv = as.data.frame(cv_tmp)
  names(result_cv) = c("accuracy_glmnet", "accuracy_boot", "mse_glmnet", "mse_boot")
  result_cv = colMeans(result_cv)
  result_df = data.frame(Model = c("Logistic-Lasso", "Bootstrap smoothing"), 
                        accuracy = c(result_cv[["accuracy_glmnet"]], result_cv[["accuracy_boot"]]),
                        MSE = c(result_cv[["mse_glmnet"]], result_cv[["mse_boot"]])ï¼Œ
                        Boot_sample = c(1,boot_n))
  result_pool = rbind(result_pool, result_df)
  print(c("finished ", boot_n))
}
result_pool = unique(result_pool) #Logistic-lasso results are repeated
knitr::kable(result_pool)
```

```{r, eval = FALSE }
result_pool %>% filter(Boot_sample > 2) %>% ggplot(aes(x = Boot_sample, y = MSE)) + geom_line() + 
  labs(title = "The bootstraped sample size for each prediction and prediction error")
```

```{r, eval = FALSE }
boot_smooth_select_feature <- function(x,y,lambda_seq,iterations=100){
  coef_boot <- foreach(m = 1:iterations,.combine = cbind, .packages = c("glmnet","tidyverse")) %dopar% {
    boot_positions <- sample(nrow(x), size = nrow(x), replace = T)
    boot_pos <- 1:nrow(x) %in% boot_positions
    y_boot = y[boot_pos]
    x_boot = x[boot_pos,]
    cv.lasso <- cv.glmnet(x_boot, y_boot, alpha = 1, family = "binomial", lambda = lambda_seq)
    # Fit the final model on the training data
    model <- glmnet(x_boot, y_boot, alpha = 1, family = "binomial",
                lambda = cv.lasso$lambda.min)
    coef(model)
  }
  approach1 = !(apply(coef_boot, 1, quantile, probs = c(0.025, 0.975)) %>% apply(.,2,is.element,0) %>% apply(.,2,any))
  approach2 = !(apply(coef_boot, 1, is.element, 0) %>% apply(.,2, any))
  approach3 = !(apply(coef_boot, 1, is.element, 0) %>% apply(.,2, all))
  approach4 = !(apply(coef_boot, 1, Mode)  %>% is.element(.,0))
  result = cbind(approach1,approach2,approach3, approach4)
  return(result)
 }
```

## Using this algorithm on the full dataset
```{r, eval = FALSE }
result = boot_smooth_select_feature(x_full,y_full, lambda_seq, iterations = 10)
knitr::kable(result)
```


