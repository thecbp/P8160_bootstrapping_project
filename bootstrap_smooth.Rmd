---
title: "bootstrap-smoothing"
author: "Junting Ren"
date: "4/6/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet)
library(doParallel)
source("LogLasso.R")
cl<-makeCluster(2) #change the 2 to your number of CPU cores  
registerDoParallel(cl)  
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
```

#Reading and cleaning the data
```{r}
down_dat = read_csv("Down.csv") %>% select(-MouseID) %>% mutate(Class = as.factor(Class)) %>% 
  na.omit() #getting rid of nas since glmnet cannot handle missing data
y_full = as.matrix(ifelse(down_dat$Class == "Control",0,1))
x_full = as.matrix(down_dat %>% select(-Class))
set.seed(1001)
#Geting the trained dataset
train_index = sample(1:nrow(x_full), ceiling(nrow(x_full) * 0.8))
y_train = y_full[train_index]
x_train = x_full[train_index,]
#getting the test dataset
y_test = y_full[-train_index]
x_test = x_full[-train_index,]
```
# 1.Apply the logistic-LASSO algorihtm your group developped in Project 2 to select features for predicting Down syndrome.
## Finding our lambda sequence
According to Regularization Paths for Generalized Linear Models via Coordinate Descent, $\lambda_{max}$ is the smallest $\lambda$ where $beta$ are 0. Our strategy is to select a minimum value $\lambda_{min} = \epsilon \lambda_{max}$, and construct a sequence of K values of $\lambda$ decreasing from  $\lambda_{max}$ to  $\lambda_{min}$. Here we let $\epsilon = 0.001$ and $K = 10$.
```{r}
#Using the built in function to get the max lambda
model_glmnet = cv.glmnet(x_train, y_train, family = "binomial")
#Getting our own lambda sequence
lambda_seq = seq(max(model_glmnet$lambda), max(model_glmnet$lambda)*0.001, length.out = 10)
```

## Fitting and predicting using our model and built in lasso regression function
```{r}
#Our model
start = Sys.time()
log_lasso = optimize(as.data.frame(x_train),y_train, lambda_seq, rep(0.01, 77))
our_model = log_lasso$finalModel
y_predict_our_model = predict(our_model,x_test)
y_response_our_model = as.numeric(y_predict_our_model > 0.5)
end = Sys.time()
#reuslts
accuracy_our_model = mean(y_response_our_model == y_test)
mse_our_model = mean((y_predict_our_model - y_test)^2)
time_our_model = end - start
lambda_our_model = log_lasso$lambda.min
#Compare with the built in lasso
start = Sys.time()
lasso_glmnet_cv = cv.glmnet(x_train, y_train, alpha = 1, family = "binomial", lambda = lambda_seq)
y_response_glmnet = predict.cv.glmnet(lasso, newx = x_test, type = "response", s = "lambda.min")
end = Sys.time()
#results
accuracy_glmnet = mean((y_response_glmnet>0.5) == y_test) 
mse_glmnet = mean((y_response_glmnet - y_test)^2)
time_glmnet = end - start
lambda_glmnet = lasso_glmnet_cv$lambda.min
```



# 2.Use the bootstrap-smoothing approach proposed in Efron (2014, JASA) to predict Down syndrome.

## Building the bootstrap-smoothing function
```{r}
boot_smooth <- function(x_train,y_train,x_test,y_test,lambda_seq,iterations=1000){
  predictions <- foreach(m = 1:iterations,.combine = cbind, .packages = "glmnet") %dopar% {
    boot_positions <- sample(nrow(x_train), size = nrow(x_train), replace = T)
    boot_pos <- 1:nrow(x_train) %in% boot_positions
    y = y_train[boot_pos]
    x = x_train[boot_pos,]
    cv.lasso <- cv.glmnet(x_train, y_train, alpha = 1, family = "binomial", lambda = lambda_seq)
    # Fit the final model on the training data
    model <- glmnet(x_train, y_train, alpha = 1, family = "binomial",
                lambda = cv.lasso$lambda.min)
    predict(model,newx = x_test, type = "response")
  }
  apply(predictions, 1, mean) #Getting the mode across the rows: majority vote 
}

```

## Prediction for bootstrap-smoothing approac
```{r}
start = Sys.time()
y_response_boot = boot_smooth(x_train,y_train,x_test,y_test, lambda_seq = lambda_seq, iterations = 100)
end = Sys.time()
#results
accuracy_boot = mean((y_response_boot > 0.5) == y_test)
mse_boot = mean((y_response_boot - y_test)^2)
time_boot = end - start
lambda_boot = NA
```

# 3.Compare the two approaches in predicting Down syndrome, and report your findings.
```{r}
#Build a result table
data.frame(model = c("our", "built in cv lasso", "bootstrap smooth"), accuracy = c(accuracy_our_model, accuracy_glmnet, accuracy_boot),
           mse = c(mse_our_model, mse_glmnet, mse_boot), time = c(time_our_model,time_glmnet,time_boot),
           lambda_min = c(lambda_our_model, lambda_glmnet, lambda_boot))
```

# 4.Using bootstrap-smoothing approach to select features
```{r}

```

