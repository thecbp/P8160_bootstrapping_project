---
title: "bootstrap-smoothing"
author: "Junting Ren"
date: "4/6/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet)
library(doParallel)
source("LogLasso.R")
cl<-makeCluster(2) #change the 2 to your number of CPU cores  
registerDoParallel(cl)  
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
```

#Reading and cleaning the data
```{r}
down_dat = read_csv("Down.csv") %>% select(-MouseID) %>% mutate(Class = as.factor(Class)) %>% 
  na.omit() #getting rid of nas since glmnet cannot handle missing data
y_full = as.matrix(ifelse(down_dat$Class == "Control",0,1))
x_full = as.matrix(down_dat %>% select(-Class))
set.seed(1001)
#Geting the trained dataset
train_index = sample(1:nrow(x_full), ceiling(nrow(x_full) * 0.9))
y_train = y_full[train_index]
x_train = x_full[train_index,]
#getting the test dataset
y_test = y_full[-train_index]
x_test = x_full[-train_index,]
```
# 1.Apply the logistic-LASSO algorihtm your group developped in Project 2 to select features for predicting Down syndrome.
## Finding our lambda sequence
According to Regularization Paths for Generalized Linear Models via Coordinate Descent, $\lambda_{max}$ is the smallest $\lambda$ where $beta$ are 0. Our strategy is to select a minimum value $\lambda_{min} = \epsilon \lambda_{max}$, and construct a sequence of K values of $\lambda$ decreasing from  $\lambda_{max}$ to  $\lambda_{min}$. Here we let $\epsilon = 0.001$ and $K = 10$.
```{r}
#Using the built in function to get the max lambda
model_glmnet = cv.glmnet(x_train, y_train, family = "binomial")
#Getting our own lambda sequence
lambda_seq = seq(max(model_glmnet$lambda), max(model_glmnet$lambda)*0.001, length.out = 10)
```

## Fitting and predicting using our model and built in lasso regression function
```{r}
#Our model
start = Sys.time()
log_lasso = optimize(as.data.frame(x_train),y_train, lambda_seq, rep(0.01, 77))
our_model = log_lasso$finalModel
y_predict_our_model = predict(our_model,x_test)
y_response_our_model = as.numeric(y_predict_our_model > 0.5)
end = Sys.time()
#reuslts
accuracy_our_model = mean(y_response_our_model == y_test)
mse_our_model = mean((y_predict_our_model - y_test)^2)
time_our_model = end - start
lambda_our_model = log_lasso$lambda.min
#Compare with the built in lasso
start = Sys.time()
lasso_glmnet_cv = cv.glmnet(x_train, y_train, alpha = 1, family = "binomial", lambda = lambda_seq)
y_response_glmnet = predict.cv.glmnet(lasso_glmnet_cv, newx = x_test, type = "response", s = "lambda.min")
end = Sys.time()
#results
accuracy_glmnet = mean((y_response_glmnet>0.5) == y_test) 
mse_glmnet = mean((y_response_glmnet - y_test)^2)
time_glmnet = end - start
lambda_glmnet = lasso_glmnet_cv$lambda.min
```


# 2.Use the bootstrap-smoothing approach proposed in Efron (2014, JASA) to predict Down syndrome.

## Building the bootstrap-smoothing function
```{r}
boot_smooth <- function(x_train,y_train,x_test,y_test,lambda_seq,iterations=1000){
  predictions <- foreach(m = 1:iterations,.combine = cbind, .packages = "glmnet") %dopar% {
    boot_positions <- sample(nrow(x_train), size = nrow(x_train), replace = T)
    boot_pos <- 1:nrow(x_train) %in% boot_positions
    y = y_train[boot_pos]
    x = x_train[boot_pos,]
    cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial", lambda = lambda_seq)
    # Fit the final model on the training data
    model <- glmnet(x, y, alpha = 1, family = "binomial",
                lambda = cv.lasso$lambda.min)
    predict(model,newx = x_test, type = "response")
  }
  apply(predictions, 1, mean) #Getting the mean
}

```

## Prediction for bootstrap-smoothing approach
```{r}
start = Sys.time()
y_response_boot = boot_smooth(x_train,y_train,x_test,y_test, lambda_seq = lambda_seq, iterations = 100)
end = Sys.time()
#results
accuracy_boot = mean((y_response_boot > 0.5) == y_test)
mse_boot = mean((y_response_boot - y_test)^2)
time_boot = end - start
lambda_boot = NA
```

## Results comparing the three methods
```{r}
#Build a result table
result_frame = data.frame(model = c("Our lasso", "Built in cv lasso", "Boot smooth mehtod"), accuracy = c(accuracy_our_model, accuracy_glmnet, accuracy_boot),
           mse = c(mse_our_model, mse_glmnet, mse_boot), time = c(time_our_model,time_glmnet,time_boot),
           lambda_min = c(lambda_our_model, lambda_glmnet, lambda_boot))
knitr::kable(result_frame)
```

# 3.Compare the two approaches in predicting Down syndrome, and report your findings.
## We are going to use 5-fold cross validation on the full dataset to compare the MSE and accuracy of the two approaches. 
```{r}
# Set the index for 5 fold cross-validation
set.seed(1000)
n_folds <- 5
folds_i <- sample(rep(1:n_folds, length.out = nrow(x_full)))
cv_tmp <- matrix(NA, nrow = n_folds, ncol = 4)
for (k in 1:n_folds) {
  #getting the train and test dataset for this fold
  test_i <- which(folds_i == k)
  x_test = x_full[test_i,]
  y_test = y_full[test_i]
  x_train = x_full[-test_i,]
  y_train = y_full[-test_i]
  
  #Lasoo
  lasso_glmnet_cv = cv.glmnet(x_train, y_train, alpha = 1, family = "binomial", lambda = lambda_seq)
  y_response_glmnet = predict.cv.glmnet(lasso_glmnet_cv, newx = x_test, type = "response", s = "lambda.min")
  #results
  accuracy_glmnet = mean((y_response_glmnet>0.5) == y_test) 
  mse_glmnet = mean((y_response_glmnet - y_test)^2)
  
  #bootstrap smooth
  y_response_boot = boot_smooth(x_train,y_train,x_test,y_test, lambda_seq = lambda_seq, iterations = 100)
  #results
  accuracy_boot = mean((y_response_boot > 0.5) == y_test)
  mse_boot = mean((y_response_boot - y_test)^2)
  
  #Building the result dataframe
  cv_tmp[k, 1] = accuracy_glmnet
  cv_tmp[k, 2] = accuracy_boot
  cv_tmp[k, 3] = mse_glmnet
  cv_tmp[k, 4] = mse_boot
}
result_cv = as.data.frame(cv_tmp)
names(result_cv) = c("accuracy_glmnet", "accuracy_boot", "mse_glmnet", "mse_boot")
result_cv = colMeans(result_cv)
result_cv
```

It seems they have the same accuracy. 

# 4.Using bootstrap-smoothing approach to select features
## The basic algorithm:  
1. Bootstrap from the full dataset  
2. Cross validation and find the smallest MSE lambda model  
3. Under this lambda model, store the coefficient for all variables.  
4. Repeat step 1 to step 3 for $B$ times
5.   
Approach 1: Built an empirical confidence interval for coefficients, if the $2.5\%$ to $97.5\%$ percentile contains 0 then we drop this variable.  
Approach 2: If any of the bootstrap coefficients contains 0, we drop this variable.  
Approach 3: If any of the bootstrap coefficients contains non zero, we select this feature.  
Approach 4: get the mode of the bootstrap coefficients, if the mode is non-zero, we select this feature.
```{r}
boot_smooth_select_feature <- function(x,y,lambda_seq,iterations=100){
  coef_boot <- foreach(m = 1:iterations,.combine = cbind, .packages = c("glmnet","tidyverse")) %dopar% {
    boot_positions <- sample(nrow(x), size = nrow(x), replace = T)
    boot_pos <- 1:nrow(x) %in% boot_positions
    y_boot = y[boot_pos]
    x_boot = x[boot_pos,]
    cv.lasso <- cv.glmnet(x_boot, y_boot, alpha = 1, family = "binomial", lambda = lambda_seq)
    # Fit the final model on the training data
    model <- glmnet(x_boot, y_boot, alpha = 1, family = "binomial",
                lambda = cv.lasso$lambda.min)
    coef(model)
  }
  approach1 = !(apply(coef_boot, 1, quantile, probs = c(0.025, 0.975)) %>% apply(.,2,is.element,0) %>% apply(.,2,any))
  approach2 = !(apply(coef_boot, 1, is.element, 0) %>% apply(.,2, any))
  approach3 = !(apply(coef_boot, 1, is.element, 0) %>% apply(.,2, all))
  approach4 = !(apply(coef_boot, 1, Mode)  %>% is.element(.,0))
  result = cbind(approach1,approach2,approach3, approach4)
  return(result)
 }
```

##Using this algorithm on the full dataset
```{r}
result = boot_smooth_select_feature(x_full,y_full, lambda_seq, iterations = 10)
knitr::kable(result)
```


