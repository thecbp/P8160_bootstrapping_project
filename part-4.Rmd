---
title: "Part 4: Making Bootstrap Inferences"
author: "Christian Pascual"
date: "4/10/2019"
output: html_document
---

# The current approach

1) Draw randomly with replacement from the data
2) Create a LASSO model from the bootstrap data
3) Store the matrix of coefficients
4) Perform the calculations needed to produce the smoothed estimate $\widetilde{\beta}$ and the smoothed standard devation $\widetilde{sd_b}$
5) Test each of the coefficients against the hypotheses:

# Code 

```{r data, message = FALSE, warning = FALSE }
library(tidyverse)
library(glmnet)
library(parallel) 
library(doParallel) 
library(foreach) 
library(iterators)

nCores = 6 # change to whatever fits your specific system
registerDoParallel(nCores)

ds = read.csv("Down.csv") %>% 
  mutate(
    class = ifelse(Class == "Control", 0, 1)) %>% 
  select(-MouseID, -Class) %>% 
  na.omit() # getting rid of nas since glmnet cannot handle missing data

ds.X = ds %>% select(-class)
ds.y = ds$class
```

```{r, helpers }
boot.count = function(idx, data) {
  counts = NULL
  for (i in 1:nrow(data)) {
    counts[i] = length(which(idx == i))
  }
  return(counts)
}
```

```{r, bootstrapping }
B = 1000
coeff.mat = NULL
Yij.mat = NULL

multiResultClass = function(res1 = NULL, res2 = NULL) {
  me = list(
    res2 = res2,
    res1 = res1
  )
  return(me)
}

bs = foreach(i = 1:B, .combine = cbind) %dopar% {
  res = multiResultClass()
               
  # Create the bootstrap sample
  boot.idx = sample(nrow(ds), size = nrow(ds), replace = T)
  boot.X = ds.X[boot.idx,] %>% as.matrix(.)
  boot.y = ds.y[boot.idx] %>% as.matrix(.)
  
  # Calculate the count matrix
  Yij = boot.count(boot.idx, boot.X)
  
  # Cross-validate for lambda and then fit this optimal model
  cv.lasso = cv.glmnet(boot.X, boot.y, alpha = 1, family = "binomial")
  best.lasso = glmnet(boot.X, boot.y, 
                      alpha = 1, lambda = cv.lasso$lambda.min)
  
  # Extract the coefficients and store in a matrix
  coeffs = matrix(coef(best.lasso))
  coeff.mat = cbind(coeff.mat, coeffs)
  Yij.mat = cbind(Yij.mat, Yij)
  
  res$res1 = coeffs
  res$res2 = Yij
  }

for (i in 1:B) {
  # Create the bootstrap sample
  boot.idx = sample(nrow(ds), size = nrow(ds), replace = T)
  boot.X = ds.X[boot.idx,] %>% as.matrix(.)
  boot.y = ds.y[boot.idx] %>% as.matrix(.)
  
  # Calculate the count matrix
  Yij = boot.count(boot.idx, boot.X)
  
  # Cross-validate for lambda and then fit this optimal model
  cv.lasso = cv.glmnet(boot.X, boot.y, alpha = 1, family = "binomial")
  best.lasso = glmnet(boot.X, boot.y, 
                      alpha = 1, lambda = cv.lasso$lambda.min)
  
  # Extract the coefficients and store in a matrix
  coeffs = matrix(coef(best.lasso))
  coeff.mat = cbind(coeff.mat, coeffs)
  Yij.mat = cbind(Yij.mat, Yij)
}

# Finishing off the calculations
Y.j = rowSums(Yij.mat)/B
t.j = rowSums(coeff.mat)/B
t.diff = coeff.mat - t.j
Yij.diff = Yij.mat - Y.j
```

```{r}
# Perform this action for each coefficient
cov.mat = NULL
for (b in 1:78) {
  covs = NULL
  for (j in 1:nrow(ds)) {
    # Each individual point will have a bootstrap
    # covariance with the bootstrap statistic
    cov.j = 0
    for (i in 1:B) {
      cov.j = cov.j + ((Yij.mat[j,i] - Y.j[j]) * (coeff.mat[b,i] - t.j[b]))/B
    }
    covs = c(covs, cov.j)
  }
  cov.mat = rbind(cov.mat, covs)
}
```

```{r}
# With the calculated covariances for each point, 
# calculate the smooth bootstrap standard deviation
sdbs = rep(0, 78)
for (i in 1:nrow(cov.mat)) {
  sdbs[i] = (sum(cov.mat[i,] * cov.mat[i,]))^(1/2)
}

# Calculate the standard bootstrap confidence intervals
standard.std = NULL
for (i in 1:nrow(cov.mat)) {
  std = sum((1/B) * (coeff.mat[i,] - mean(coeff.mat[i,]))^2)^(1/2)
  standard.std = c(standard.std, std)
}

# Calculate the percentile confidence intervals
p025 = NULL
p975 = NULL
for (i in 1:nrow(cov.mat)) {
  p025 = c(p025, quantile(t.diff[i,], 0.025))
  p975 = c(p975, quantile(t.diff[i,], 0.975))
}

conf.mat = as_tibble(cbind(t.j, sdbs, standard.std, p025, p975))
```

```{r}
conf.mat %>% 
  mutate(idx = 1:nrow(conf.mat)) %>% 
  ggplot(data = ., aes(x = idx, y = t.j)) +
  geom_point() +
  geom_errorbar(ymin = t.j - qnorm(0.025)*sdbs,
                ymax = t.j - qnorm(0.975)*sdbs, 
                color = "red", alpha = 0.3) +
  geom_errorbar(aes(x = idx + 0.3),
                ymin = p025,
                ymax = p975, 
                color = "blue", alpha = 0.3) +
  geom_errorbar(ymin = t.j - qnorm(0.025) * standard.std,
                ymax = t.j + qnorm(0.025) * standard.std, 
                color = "green", alpha = 0.3)
```

